From 6ddad7ae3d0721f77ae46146d58cd9dc6c772808 Mon Sep 17 00:00:00 2001
From: andreyan <andrey.anufriev@intel.com>
Date: Thu, 27 Aug 2020 09:16:47 +0300
Subject: [PATCH] Added batch norm fusing to conv layers

---
 models/fatchord_version.py | 83 ++++++++++++++++++++++++++++++++++++++++------
 1 file changed, 72 insertions(+), 11 deletions(-)

diff --git a/models/fatchord_version.py b/models/fatchord_version.py
index d43e6ad..e1c46d1 100644
--- a/models/fatchord_version.py
+++ b/models/fatchord_version.py
@@ -10,6 +10,28 @@ from pathlib import Path
 from typing import Union
 
 
+def fuse(conv, bn):
+    w = conv.weight
+    mean = bn.running_mean
+    var_sqrt = torch.sqrt(bn.running_var + bn.eps)
+    beta = bn.weight
+    gamma = bn.bias
+    if conv.bias is not None:
+        b = conv.bias
+    else:
+        b = mean.new_zeros(mean.shape)
+    w = w * (beta / var_sqrt).reshape([conv.out_channels, 1, 1])
+    b = (b - mean)/var_sqrt * beta + gamma
+    fused_conv = nn.Conv1d(conv.in_channels,
+                         conv.out_channels,
+                         conv.kernel_size,
+                         conv.stride,
+                         conv.padding,
+                         bias=True)
+    fused_conv.weight = nn.Parameter(w)
+    fused_conv.bias = nn.Parameter(b)
+    return fused_conv
+
 class ResBlock(nn.Module):
     def __init__(self, dims):
         super().__init__()
@@ -17,16 +39,28 @@ class ResBlock(nn.Module):
         self.conv2 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)
         self.batch_norm1 = nn.BatchNorm1d(dims)
         self.batch_norm2 = nn.BatchNorm1d(dims)
+        self.fused = False
 
     def forward(self, x):
         residual = x
-        x = self.conv1(x)
-        x = self.batch_norm1(x)
-        x = F.relu(x)
-        x = self.conv2(x)
-        x = self.batch_norm2(x)
+
+        if self.fused:
+            x = self.conv1_f(x)
+            x = F.relu(x)
+            x = self.conv2_f(x)
+        else:
+            x = self.conv1(x)
+            x = self.batch_norm1(x)
+            x = F.relu(x)
+            x = self.conv2(x)
+            x = self.batch_norm2(x)
         return x + residual
 
+    def fuse(self):
+        self.conv1_f = fuse(self.conv1, self.batch_norm1)
+        self.conv2_f = fuse(self.conv2, self.batch_norm2)
+        self.fused = True
+
 
 class MelResNet(nn.Module):
     def __init__(self, res_blocks, in_dims, compute_dims, res_out_dims, pad):
@@ -38,15 +72,26 @@ class MelResNet(nn.Module):
         for i in range(res_blocks):
             self.layers.append(ResBlock(compute_dims))
         self.conv_out = nn.Conv1d(compute_dims, res_out_dims, kernel_size=1)
+        self.fused = False
 
     def forward(self, x):
-        x = self.conv_in(x)
-        x = self.batch_norm(x)
+        if self.fused:
+            x = self.conv_in_f(x)
+        else:
+            x = self.conv_in(x)
+            x = self.batch_norm(x)
         x = F.relu(x)
         for f in self.layers: x = f(x)
         x = self.conv_out(x)
         return x
 
+    def fuse(self):
+        self.conv_in_f = fuse(self.conv_in, self.batch_norm)
+        self.fused = True
+
+        for l in self.layers:
+            l.fuse()
+
 
 class Stretch2d(nn.Module):
     def __init__(self, x_scale, y_scale):
@@ -61,6 +106,16 @@ class Stretch2d(nn.Module):
         return x.view(b, c, h * self.y_scale, w * self.x_scale)
 
 
+class StretchUpsample(nn.Module):
+    def __init__(self, x_scale):
+        super().__init__()
+        self.upsample = nn.Upsample(scale_factor=x_scale)
+
+    def forward(self, x):
+        print('USHAPE: ', x.shape)
+        return self.upsample(x)
+
+
 class UpsampleNetwork(nn.Module):
     def __init__(self, feat_dims, upsample_scales, compute_dims,
                  res_blocks, res_out_dims, pad):
@@ -68,12 +123,12 @@ class UpsampleNetwork(nn.Module):
         total_scale = np.cumproduct(upsample_scales)[-1]
         self.indent = pad * total_scale
         self.resnet = MelResNet(res_blocks, feat_dims, compute_dims, res_out_dims, pad)
-        self.resnet_stretch = Stretch2d(total_scale, 1)
+        self.resnet_stretch = StretchUpsample((1,total_scale)) #Stretch2d(total_scale, 1)
         self.up_layers = nn.ModuleList()
         for scale in upsample_scales:
             k_size = (1, scale * 2 + 1)
             padding = (0, scale)
-            stretch = Stretch2d(scale, 1)
+            stretch = StretchUpsample((1, scale)) # Stretch2d(scale, 1)
             conv = nn.Conv2d(1, 1, kernel_size=k_size, padding=padding, bias=False)
             conv.weight.data.fill_(1. / k_size[1])
             self.up_layers.append(stretch)
@@ -84,10 +139,15 @@ class UpsampleNetwork(nn.Module):
         aux = self.resnet_stretch(aux)
         aux = aux.squeeze(1)
         m = m.unsqueeze(1)
-        for f in self.up_layers: m = f(m)
+        for i, f in enumerate(self.up_layers):
+            m = f(m)
         m = m.squeeze(1)[:, :, self.indent:-self.indent]
+
         return m.transpose(1, 2), aux.transpose(1, 2)
 
+    def fuse(self):
+        self.resnet.fuse()
+
 
 class WaveRNN(nn.Module):
     def __init__(self, rnn_dims, fc_dims, bits, pad, upsample_factors,
@@ -422,7 +482,7 @@ class WaveRNN(nn.Module):
         # of the model itself. Let caller take care of saving optimzier state.
         torch.save(self.state_dict(), path)
 
-    def num_params(self, print_out=False):
+    def num_params(self, print_out=True):
         parameters = filter(lambda p: p.requires_grad, self.parameters())
         parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000
         if print_out:
@@ -433,3 +493,4 @@ class WaveRNN(nn.Module):
         """Calls `flatten_parameters` on all the rnns used by the WaveRNN. Used
         to improve efficiency and avoid PyTorch yelling at us."""
         [m.flatten_parameters() for m in self._to_flatten]
+
-- 
2.7.4

